import itertools
import json
import logging
import os
from ast import literal_eval
from collections import defaultdict
from typing import Dict, List, Mapping, Tuple
import networkx as nx

from eth_utils import keccak

from greed.block import Block
from greed.factory import Factory
from greed.function import TAC_Function
from greed.TAC import TAC_Statement, tac_opcode_to_class_map
from greed.TAC.gigahorse_ops import TAC_Nop
from greed.TAC.special_ops import TAC_Stop
from greed.utils.files import load_csv, load_csv_map, load_csv_multimap

log = logging.getLogger(__name__)

class TAC_parser:
    """
    This class parses the TAC facts generated by Gigahorse and builds the CFG.
    """
    factory: Factory
    target_dir: str

    statement_to_blocks_map: Mapping[str, List[str]]


    def __init__(self, factory: Factory, target_dir: str):
        self.factory = factory
        self.target_dir = target_dir

        self.statement_to_blocks_map = None

    @staticmethod
    def stmt_sort_key(stmt_id: str) -> int:
        """
        Statements may be identified by a number of different formats. This function
        returns a key that can be used to sort them in a consistent way.

        Possible formats:
        - 0x12345
        - 0x12345_0x456
        - 0x12345S0x456
        """
        if '_' in stmt_id:
            assert 'S' not in stmt_id
            return int(stmt_id.split('_')[1], base=16)
        elif 'S' in stmt_id:
            assert '_' not in stmt_id
            return int(stmt_id.split('0x')[1].split('S')[0], base=16)
        else:
            return int(stmt_id.split('0x')[1], base=16)
        

    def parse_statements(self) -> Dict[str, TAC_Statement]:
        # Load facts
        tac_function_blocks = load_csv_multimap(f"{self.target_dir}/InFunction.csv", reverse=True)
        tac_block_stmts = load_csv_multimap(f"{self.target_dir}/TAC_Block.csv", reverse=True)
        tac_op = load_csv_map(f"{self.target_dir}/TAC_Op.csv")

        self.statement_to_blocks_map = load_csv_multimap(f"{self.target_dir}/TAC_OriginalStatement_Block.csv")

        tac_variable_value = load_csv_map(f"{self.target_dir}/TAC_Variable_Value.csv")
        tac_variable_value = {'v' + v.replace('0x', ''): val for v, val in tac_variable_value.items()}

        tac_defs: Mapping[str, List[Tuple[str, int]]] = defaultdict(list)
        for stmt_id, var, pos in load_csv(f"{self.target_dir}/TAC_Def.csv"):
            tac_defs[stmt_id].append((var, int(pos)))

        tac_uses: Mapping[str, List[Tuple[str, int]]] = defaultdict(list)
        for stmt_id, var, pos in load_csv(f"{self.target_dir}/TAC_Use.csv"):
            tac_uses[stmt_id].append((var, int(pos)))
        
        func_name_to_sig = load_csv(f"{self.target_dir}/ConstantPossibleSigHash.csv")
        func_name_to_sig = {x: y for x, y in zip(*list(zip(*func_name_to_sig))[-2:])}
        # Entries with a signature longer than 10 characters are most likely false positives of the analysis, not safe to import.
        func_name_to_sig = {name:"0x"+signature[2:].zfill(8) for signature, name in func_name_to_sig.items() if len(signature) <= 10}

        fixed_calls : Mapping[str, List[Tuple[str, str]]] = defaultdict(list)

        for stmt_id, func_target in load_csv(f"{self.target_dir}/CallToSignature.csv"):
            # We want to skip the "LOCKXXX" target and keep only the one
            # that Gigahorse successfully resolved
            if "LOCK" in func_target: continue
            if func_target in func_name_to_sig:
                fixed_calls[stmt_id] = func_name_to_sig[func_target]
            else:
                fixed_calls[stmt_id] = "0x"

        
        for stmt_id, func_target in load_csv(f"{self.target_dir}/CallToSignatureFromSHA3.csv"):
            if "LOCK" in func_target: continue
            if func_target in func_name_to_sig:
                fixed_calls[stmt_id] = func_name_to_sig[func_target]
            else:
                fixed_calls[stmt_id] = "0x"
        
        # parse all statements block after block
        statements: Dict[str, TAC_Statement] = dict()
        for block_id in itertools.chain(*tac_function_blocks.values()):
            for stmt_id in tac_block_stmts[block_id]:
                opcode = tac_op[stmt_id]
                raw_uses = [var for var, _ in sorted(tac_uses[stmt_id], key=lambda x: x[1])]
                raw_defs = [var for var, _ in sorted(tac_defs[stmt_id], key=lambda x: x[1])]
                uses = ['v' + v.replace('0x', '') for v in raw_uses]
                defs = ['v' + v.replace('0x', '') for v in raw_defs]
                values = {v: tac_variable_value.get(v, None) for v in uses + defs}
                OpcodeClass = tac_opcode_to_class_map[opcode]
                statement = OpcodeClass(block_id=block_id, stmt_id=stmt_id, uses=uses, defs=defs, values=values)
                statements[stmt_id] = statement
                
                # Adding metadata for CALL statements
                if stmt_id in fixed_calls:
                    log.debug(f"Setting {statement} as fixed call to {fixed_calls[stmt_id]}")
                    statement.set_likely_known_target_func(fixed_calls[stmt_id])

            if not tac_block_stmts[block_id]:
                # Gigahorse sometimes creates empty basic blocks. If so, inject a NOP statement
                fake_stmt = TAC_Nop(block_id=block_id, stmt_id=block_id + '_fake_stmt')
                statements[block_id + '_fake_stmt'] = fake_stmt

        # inject a fake exit statement to simplify the handling of CALLPRIVATE without successors
        fake_exit_stmt = TAC_Stop(block_id='fake_exit', stmt_id='fake_exit')
        statements['fake_exit'] = fake_exit_stmt

        return statements

    def parse_blocks(self) -> Dict[str, Block]:
        # Load facts
        tac_function_blocks = load_csv_multimap(f"{self.target_dir}/InFunction.csv", reverse=True)
        tac_block_stmts = load_csv_multimap(f"{self.target_dir}/TAC_Block.csv", reverse=True)
        tac_fallthrough_edge = load_csv_map(f"{self.target_dir}/IRFallthroughEdge.csv")
        tac_statement_nexts = load_csv_multimap(f"{self.target_dir}/TAC_Statement_Next.csv")

        # parse all blocks
        blocks: Dict[str, Block] = dict()
        for block_id in itertools.chain(*tac_function_blocks.values()):
            statements: List[TAC_Statement] = list()
            
            g = nx.DiGraph() # for next-statement edges

            for stmt_id in tac_block_stmts[block_id]:
                statement = self.factory.statement(stmt_id)

                # remove assertions for PHI statements
                statements.append(statement)

                next_stmt_ids = tac_statement_nexts.get(stmt_id, [])

                g.add_node(stmt_id)
                if len(next_stmt_ids) == 1:
                    # if more than one next statement, ignore (probably end-of-block jumpi)
                    next_stmt_id = next_stmt_ids[0]
                    g.add_edge(stmt_id, next_stmt_id)


            if len(statements) == 0:
                # Gigahorse sometimes creates empty basic blocks. If so, inject a NOP
                fake_stmt = self.factory.statement(block_id + '_fake_stmt')
                statements.append(fake_stmt)
            else:
                # There exist some statements

                #
                # Walk backward to the root of the block, i.e., the first statement
                root = statements[0].id
                while len(list(g.predecessors(root))) > 0:
                    root = list(g.predecessors(root))[0]

                # Ensure that all statements in the block are reachable from the root
                reachable = set(nx.descendants(g, root))
                reachable.add(root)
                block_stmt_ids = set([s.id for s in statements])
                assert block_stmt_ids.issubset(reachable), f"Block {block_id} has unreachable statements"

                # Order statements by topological order
                topo_sort = list(nx.topological_sort(g))
                statements = sorted(statements, key=lambda s: topo_sort.index(s.id))

            blocks[block_id] = Block(block_id=block_id, statements=statements)

        # set fallthrough edge
        for block_id in blocks:
            fallthrough_block_id = tac_fallthrough_edge.get(block_id, None)
            if fallthrough_block_id is not None:
                blocks[block_id].fallthrough_edge = blocks[fallthrough_block_id]

        # inject a fake exit block to simplify the handling of CALLPRIVATE without successors
        fake_exit_stmt = self.factory.statement('fake_exit')
        fake_exit_block = Block(block_id='fake_exit', statements=[fake_exit_stmt])
        fake_exit_block._succ = fake_exit_block._pred = fake_exit_block._ancestors = fake_exit_block._descendants = []
        blocks['fake_exit'] = fake_exit_block

        return blocks


    def parse_functions(self) -> Dict[str, TAC_Function]:
        # Load facts
        tac_block_function = load_csv_map(f"{self.target_dir}/InFunction.csv")
        tac_function_blocks = load_csv_multimap(f"{self.target_dir}/InFunction.csv", reverse=True)
        tac_func_id_to_public = load_csv_map(f"{self.target_dir}/PublicFunction.csv")
        tac_high_level_func_name = load_csv_map(f"{self.target_dir}/HighLevelFunctionName.csv")
        tac_block_succ = load_csv_multimap(f"{self.target_dir}/LocalBlockEdge.csv")
        tac_function_entry = load_csv(f"{self.target_dir}/IRFunctionEntry.csv")
        
        tac_formal_args: Mapping[str, List[Tuple[str, int]]] = defaultdict(list)
        for func_id, arg, pos in load_csv(f"{self.target_dir}/FormalArgs.csv"):
            tac_formal_args[func_id].append((arg, int(pos)))

        functions: Dict[str, TAC_Function] = dict()
        for block_id, in tac_function_entry:
            func_id = tac_block_function[block_id]
            is_public = func_id in tac_func_id_to_public or func_id == '0x0'
            is_fallback = tac_func_id_to_public.get(func_id, None) == '0x0'
            signature = tac_func_id_to_public.get(func_id, None)

            # Pad signature over 4 bytes
            if signature:
                signature = "0x"+signature[2:].zfill(8)

            high_level_name = 'fallback()' if is_fallback else tac_high_level_func_name[func_id]

            formals = [var for var, _ in sorted(tac_formal_args[func_id], key=lambda x: x[1])]

            blocks = [self.factory.block(id) for id in tac_function_blocks[func_id]]

            function = TAC_Function(block_id, signature, high_level_name, is_public, blocks, formals)
            function.cfg = function.build_cfg(self.factory, tac_block_succ)
            function.build_use_def_graph()
            functions[func_id] = function

            for b in blocks:
                b.function = function

        # rewrite aliases according to PHI map
        translate_alias = lambda alias: 'v' + alias.replace('0x', '')
        for function in functions.values():
            function.arguments = [translate_alias(a) for a in function.arguments]

        return functions
    
    def parse_blocks_in_loop(self):
        blocks_in_loop = load_csv_multimap(f"{self.target_dir}/BlockInStructuredLoop.csv", reverse=True)
        return blocks_in_loop
    
    def parse_induction_variables(self):
        induction_variables = load_csv_multimap(f"{self.target_dir}/InductionVariable.csv", reverse=True)
        induction_variables = {x: {'v' + _y.replace('0x', '') for _y in y}
                               for x, y in induction_variables.items()}
        return induction_variables
    
    def parse_induction_variable_starts_at_const(self):
        values = load_csv(f"{self.target_dir}/InductionVariableStartsAtConst.csv")
        starts_at_const = defaultdict(dict)
        for _x, _y, _z in values:
            y = _y[1:-1].split(", ")[1]
            y = 'v' + y.replace('0x', '')
            starts_at_const[_x][y] = literal_eval(_z) # seems to not have a consistent base (either 10 or 16)
        return starts_at_const
    
    def parse_induction_variable_increases_by_const(self):
        values = load_csv(f"{self.target_dir}/InductionVariableIncreasesByConst.csv")
        increases_by_const = defaultdict(dict)
        for _x, _y, _z in values:
            y = _y[1:-1].split(", ")[1]
            y = 'v' + y.replace('0x', '')
            increases_by_const[_x][y] = literal_eval(_z) # seems to not have a consistent base (either 10 or 16)
        return increases_by_const
    
    def parse_induction_variable_upper_bounds(self):
        values = load_csv(f"{self.target_dir}/InductionVariableUpperBoundVar.csv")
        upper_bounds = defaultdict(dict)
        for _x, _y, _z in values:
            y = _y[1:-1].split(", ")[1]
            y = 'v' + y.replace('0x', '')
            upper_bounds[_x][y] = 'v' + _z.replace('0x', '')
        return upper_bounds

    def parse_guarding_slots(self):
        tac_guarded_blocks = load_csv_multimap(f"{self.target_dir}/StaticallyGuardedBlock.csv")
        guarding_slots = set([g.split('_')[0] for g_list in tac_guarded_blocks.values() for g in g_list if g.startswith('0x')])
        return guarding_slots
    
    def parse_guarded_slots(self, guarding_slots, sstores_for_slot):
        tac_guarded_blocks = load_csv_multimap(f"{self.target_dir}/StaticallyGuardedBlock.csv")
        all_guarded_blocks = [id for id, guards in tac_guarded_blocks.items() if any(g in guarding_slots for g in guards)]
        all_slots = {slot for slot in sstores_for_slot.keys()}
        guarded_slots = set()
        for slot in all_slots:
            access_guarded = [(s.block_id in all_guarded_blocks) for s in sstores_for_slot[slot]]
            if all(access_guarded):
                guarded_slots.add(slot)
        return guarded_slots


    def parse_abi(self):
        if not os.path.exists(f"{self.target_dir}/abi.json"):
            return None
        with open(f"{self.target_dir}/abi.json", "rb") as abi_file:
            abi = json.load(abi_file)

        sig_to_name = {}
        funcs = [e for e in abi if e['type'] == 'function']
        for f in funcs:
            f_proto = f['name'] + '(' + ",".join([i['internalType'] for i in f['inputs']]) + ')'
            hexdigest = bytes(keccak(f_proto.encode('utf-8'))).hex()
            sig_to_name[f"0x{hexdigest[0:8]}"] = f_proto

        # Set the function names
        for f in self.factory.project.function_at.values():
            if f.signature in sig_to_name.keys():
                f.name = sig_to_name[f.signature]

        return abi

    # The r_abi json is expected to be a dictionary of this kind:
    # <function_id>:<function_prototype>
    #
    # e.g.
    # {"0xd450e04c":"0xd450e04c(bytes,bytes,bytes,bytes,bytes)"  
    #
    def parse_recovered_abi(self):
        if not os.path.exists(f"{self.target_dir}/r_abi.json"):
            return None
        
        log.warning("Working with an automatically recovered abi")

        with open(f"{self.target_dir}/r_abi.json", "rb") as abi_file:
            abi = json.load(abi_file)

        for f in self.factory.project.function_at.values():
            proto = abi.get(f.signature, None) 
            if proto:
                f.name = proto
        
        return abi